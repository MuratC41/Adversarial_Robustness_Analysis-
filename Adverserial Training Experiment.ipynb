{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "H√úCRE 1: K√ºt√ºphaneler, Ayarlar ve Veri Hazƒ±rlƒ±ƒüƒ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî• Cihaz: cuda\n",
            "‚úÖ Veri setleri hazƒ±rlandƒ±.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Cihaz Ayarƒ±\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üî• Cihaz: {device}\")\n",
        "\n",
        "# 2. CIFAR-10 ƒ∞statistikleri ve Fonksiyonlar\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "mean_tensor = torch.tensor(cifar10_mean, device=device).view(1, 3, 1, 1)\n",
        "std_tensor  = torch.tensor(cifar10_std,  device=device).view(1, 3, 1, 1)\n",
        "\n",
        "def normalize(x):\n",
        "    return (x - mean_tensor) / std_tensor\n",
        "\n",
        "def denormalize(x):\n",
        "    return x * std_tensor + mean_tensor\n",
        "\n",
        "# 3. Veri Y√ºkleyiciler (DataLoaders)\n",
        "def get_loaders(batch_size=128):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    ])\n",
        "    \n",
        "    train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
        "    test_ds = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
        "    \n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    \n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Klas√∂r kontrol√º\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "train_loader, test_loader = get_loaders()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(\"‚úÖ Veri setleri hazƒ±rlandƒ±.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "H√úCRE 2: Model Mimarisi (WideResNet-28-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ WideResNet-28-10 modeli hazƒ±r (H√úCRE 2).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---------------------------\n",
        "# WideResNet building blocks\n",
        "# ---------------------------\n",
        "\n",
        "class WideBasic(nn.Module):\n",
        "    def __init__(self, in_planes, planes, dropout_rate=0.0, stride=1):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = out + self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, num_layers, in_planes, out_planes, block, dropout_rate, stride):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            s = stride if i == 0 else 1\n",
        "            inp = in_planes if i == 0 else out_planes\n",
        "            layers.append(block(inp, out_planes, dropout_rate, s))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    WideResNet for CIFAR.\n",
        "    depth must satisfy: depth = 6n + 4\n",
        "\n",
        "    WRN-28-10 => depth=28 -> n=4, widen_factor=10\n",
        "    \"\"\"\n",
        "    def __init__(self, depth=28, widen_factor=10, dropout_rate=0.0, num_classes=10):\n",
        "        super().__init__()\n",
        "        assert (depth - 4) % 6 == 0, \"WideResNet depth must be 6n+4 (e.g., 28, 34, 40...)\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.block1 = NetworkBlock(n, nStages[0], nStages[1], WideBasic, dropout_rate, stride=1)\n",
        "        self.block2 = NetworkBlock(n, nStages[1], nStages[2], WideBasic, dropout_rate, stride=2)\n",
        "        self.block3 = NetworkBlock(n, nStages[2], nStages[3], WideBasic, dropout_rate, stride=2)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(nStages[3])\n",
        "        self.fc = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "        # Initialization (common WRN init)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Factory function (clean usage)\n",
        "# ---------------------------\n",
        "def make_wrn28_10(num_classes=10, dropout_rate=0.0):\n",
        "    \"\"\"\n",
        "    Returns WRN-28-10 for CIFAR-10/100.\n",
        "    dropout_rate default=0.0 (AT i√ßin ba≈ülangƒ±√ßta √∂nerilir)\n",
        "    \"\"\"\n",
        "    return WideResNet(depth=28, widen_factor=10, dropout_rate=dropout_rate, num_classes=num_classes)\n",
        "\n",
        "\n",
        "print(\"‚úÖ WideResNet-28-10 modeli hazƒ±r (H√úCRE 2).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "H√úCRE 3: Saldƒ±rƒ± (Attack) ve Deƒüerlendirme Fonksiyonlarƒ±\n",
        "FGSM / R-FGSM, PGD, TRADES ve evaluation fonksiyonlarƒ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ FGSM / PGD saldƒ±rƒ± fonksiyonlarƒ± hazƒ±r.\n",
            "‚úÖ Saldƒ±rƒ± ve Test fonksiyonlarƒ± hazƒ±r (FGSM/R-FGSM + PGD + TRADES).\n"
          ]
        }
      ],
      "source": [
        "# H√úCRE 3: Saldƒ±rƒ± (Attack) ve Deƒüerlendirme Fonksiyonlarƒ±\n",
        "# FGSM / R-FGSM, PGD, TRADES ve evaluation fonksiyonlarƒ±\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- EVALUATION ---\n",
        "@torch.no_grad()\n",
        "def evaluate_clean(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def evaluate_robust(model, loader, attack_fn, epsilon, **kwargs):\n",
        "    \"\"\"\n",
        "    Verilen attack_fn kullanƒ±larak robust accuracy hesaplar.\n",
        "    attack_fn imzasƒ±:\n",
        "        attack_fn(model, x, y, epsilon, device, **kwargs)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x_adv = attack_fn(model, x, y, epsilon=epsilon, device=device, **kwargs)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_adv)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += preds.eq(y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# --- CLAMP (Sƒ±kƒ±≈ütƒ±rma) Fonksiyonu ---\n",
        "# Normalize edilmi≈ü veriyi [0, 1] aralƒ±ƒüƒ±na denk gelen min/max deƒüerlerinde tutar.\n",
        "def clamp(x, min_val=0.0, max_val=1.0):\n",
        "    \"\"\"\n",
        "    Pratik √ß√∂z√ºm:\n",
        "      1) Denormalize et\n",
        "      2) [0,1] aralƒ±ƒüƒ±nda clamp et\n",
        "      3) Tekrar normalize et\n",
        "    \"\"\"\n",
        "    x_denorm = denormalize(x)\n",
        "    x_clamped = torch.clamp(x_denorm, min_val, max_val)\n",
        "    return normalize(x_clamped)\n",
        "\n",
        "\n",
        "# --- ATTACKS ---\n",
        "\n",
        "def fgsm_attack(model, x, y, epsilon, device, random_start=False, **kwargs):\n",
        "    \"\"\"\n",
        "    FGSM / R-FGSM saldƒ±rƒ±sƒ±.\n",
        "    \n",
        "    Parametreler:\n",
        "        random_start = False  -> Klasik FGSM\n",
        "        random_start = True   -> R-FGSM (√∂nce epsilon topu i√ßinde rastgele ba≈üla, sonra FGSM)\n",
        "    \"\"\"\n",
        "    x = x.clone().detach().to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Epsilon'u normalize uzaya √∂l√ßekle\n",
        "    eps_norm = (epsilon / std_tensor).to(device)\n",
        "\n",
        "    # Random start (R-FGSM)\n",
        "    if random_start and epsilon > 0:\n",
        "        noise = torch.empty_like(x).uniform_(-1, 1) * eps_norm\n",
        "        x = clamp(x + noise)\n",
        "\n",
        "    x.requires_grad = True\n",
        "    \n",
        "    output = model(x)\n",
        "    loss = F.cross_entropy(output, y)\n",
        "    \n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    data_grad = x.grad.data\n",
        "    \n",
        "    # Attack Step\n",
        "    x_adv = x + eps_norm * data_grad.sign()\n",
        "    \n",
        "    # Clipping (veriyi ge√ßerli aralƒ±kta tut)\n",
        "    x_adv = clamp(x_adv)\n",
        "    \n",
        "    return x_adv.detach()\n",
        "\n",
        "\n",
        "def pgd_attack(model, x, y, epsilon, device, alpha=2/255, steps=7, **kwargs):\n",
        "    \"\"\"\n",
        "    L‚àû-PGD saldƒ±rƒ±sƒ±.\n",
        "    \"\"\"\n",
        "    x = x.clone().detach().to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    eps_norm = (epsilon / std_tensor).to(device)\n",
        "    alpha_norm = (alpha / std_tensor).to(device)\n",
        "    \n",
        "    # Random Start (epsilon topu i√ßinde)\n",
        "    delta = torch.zeros_like(x).uniform_(-1, 1) * eps_norm\n",
        "    delta = clamp(x + delta) - x  # ge√ßerli aralƒ±kta kal\n",
        "    x_adv = x + delta\n",
        "    \n",
        "    for _ in range(steps):\n",
        "        x_adv.requires_grad = True\n",
        "        output = model(x_adv)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "        \n",
        "        model.zero_grad()  # gradient temizle\n",
        "        loss.backward()\n",
        "        grad = x_adv.grad.data\n",
        "        \n",
        "        # Gradient adƒ±mƒ±\n",
        "        x_adv = x_adv.detach() + alpha_norm * grad.sign()\n",
        "        \n",
        "        # Projection & Clamping\n",
        "        delta = x_adv - x\n",
        "        delta = torch.max(torch.min(delta, eps_norm), -eps_norm)\n",
        "        x_adv = clamp(x + delta)\n",
        "        \n",
        "    return x_adv.detach()\n",
        "\n",
        "\n",
        "print(\"‚úÖ FGSM / PGD saldƒ±rƒ± fonksiyonlarƒ± hazƒ±r.\")\n",
        "\n",
        "\n",
        "def trades_loss(model, x, y, epsilon, device, step_size=2/255, steps=10, beta=6.0):\n",
        "    \"\"\"\n",
        "    TRADES loss (natural CE + beta * KL(clean || adv)).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    batch_size = x.shape[0]\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    eps_norm = (epsilon / std_tensor).to(device)\n",
        "    step_norm = (step_size / std_tensor).to(device)\n",
        "    \n",
        "    # K√º√ß√ºk g√ºr√ºlt√º ile ba≈üla\n",
        "    x_adv = x.detach() + 0.001 * torch.randn_like(x).detach().to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits_clean = model(x)\n",
        "    \n",
        "    for _ in range(steps):\n",
        "        x_adv.requires_grad = True\n",
        "        logits_adv = model(x_adv)\n",
        "        loss_kl = F.kl_div(\n",
        "            F.log_softmax(logits_adv, dim=1),\n",
        "            F.softmax(logits_clean, dim=1),\n",
        "            reduction='sum'\n",
        "        )\n",
        "        grad = torch.autograd.grad(loss_kl, x_adv)[0]\n",
        "        \n",
        "        # Gradient adƒ±mƒ±\n",
        "        x_adv = x_adv.detach() + step_norm * grad.sign()\n",
        "        \n",
        "        # Projection & (istersen clamp)\n",
        "        delta = x_adv - x\n",
        "        delta = torch.max(torch.min(delta, eps_norm), -eps_norm)\n",
        "        x_adv = x + delta\n",
        "        x_adv = clamp(x_adv)\n",
        "        \n",
        "    model.train()\n",
        "    x_adv = x_adv.detach()\n",
        "    \n",
        "    logits_adv = model(x_adv)\n",
        "    logits_clean = model(x)\n",
        "    \n",
        "    loss_natural = F.cross_entropy(logits_clean, y)\n",
        "    loss_robust = (1.0 / batch_size) * F.kl_div(\n",
        "        F.log_softmax(logits_adv, dim=1),\n",
        "        F.softmax(logits_clean, dim=1),\n",
        "        reduction='sum'\n",
        "    )\n",
        "    return loss_natural + beta * loss_robust\n",
        "\n",
        "\n",
        "print(\"‚úÖ Saldƒ±rƒ± ve Test fonksiyonlarƒ± hazƒ±r (FGSM/R-FGSM + PGD + TRADES).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "H√úCRE 4: Curriculum Engine (WRN-28-10 uyumlu, PGD-7 train + PGD-20 eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ H√úCRE 4 hazƒ±r: WRN-28-10 Curriculum PGD Engine.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate_detailed(model, loader, device):\n",
        "    \"\"\"\n",
        "    Clean + FGSM + PGD-20 sanity check (epsilon=8/255)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    clean_acc = evaluate_clean(model, loader)\n",
        "\n",
        "    fgsm_acc = evaluate_robust(\n",
        "        model, loader, fgsm_attack,\n",
        "        epsilon=8/255,\n",
        "        # random_start=True  # ƒ∞stersen R-FGSM sanity check i√ßin a√ß\n",
        "    )\n",
        "\n",
        "    pgd20_acc = evaluate_robust(\n",
        "        model, loader, pgd_attack,\n",
        "        epsilon=8/255,\n",
        "        steps=20,\n",
        "        alpha=2/255\n",
        "    )\n",
        "\n",
        "    print(f\"    üìä [Detaylƒ± Test] Clean: %{clean_acc*100:.2f} | FGSM: %{fgsm_acc*100:.2f} | PGD-20: %{pgd20_acc*100:.2f}\")\n",
        "    return clean_acc, fgsm_acc, pgd20_acc\n",
        "\n",
        "\n",
        "def get_curriculum_epsilon(epoch):\n",
        "    \"\"\"\n",
        "    A≈üama 2‚Äôdeki schedule ile aynƒ±:\n",
        "      1-9   : 0\n",
        "      10-24 : 2/255\n",
        "      25-44 : 4/255\n",
        "      45-64 : 6/255\n",
        "      65-80 : 8/255\n",
        "    \"\"\"\n",
        "    if epoch <= 9:\n",
        "        return 0.0, \"üü¢ Warm-up (Clean)\"\n",
        "    elif epoch <= 24:\n",
        "        return 2/255, \"üü° Low Noise (2/255)\"\n",
        "    elif epoch <= 44:\n",
        "        return 4/255, \"üü† Mid Noise (4/255)\"\n",
        "    elif epoch <= 64:\n",
        "        return 6/255, \"üî¥ High Noise (6/255)\"\n",
        "    else:\n",
        "        return 8/255, \"üî• Target Noise (8/255)\"\n",
        "\n",
        "\n",
        "def train_curriculum_wrn(\n",
        "    model_name=\"wrn28_10_curriculum_v1\",\n",
        "    epochs=80,\n",
        "    lr=0.1,\n",
        "    weight_decay=5e-4,\n",
        "    pgd_steps_train=7,\n",
        "):\n",
        "    print(f\"\\nüöÄ CURRICULUM TRAINING BA≈ûLIYOR: {model_name}\")\n",
        "    print(f\"   Model: WRN-28-10 | Epochs: {epochs} | PGD-{pgd_steps_train} train | PGD-20 eval\")\n",
        "\n",
        "    # Model\n",
        "    model = make_wrn28_10(num_classes=10, dropout_rate=0.0).to(device)\n",
        "\n",
        "    # Optimizer + Scheduler (controlled scaling)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40, 60], gamma=0.1)\n",
        "\n",
        "    # Checkpoints\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    path_best_clean = f\"checkpoints/{model_name}_best_clean.pth\"\n",
        "    path_best_robust = f\"checkpoints/{model_name}_best_robust.pth\"\n",
        "    path_final = f\"checkpoints/{model_name}_final.pth\"\n",
        "\n",
        "    best_clean_acc = 0.0\n",
        "    best_robust_acc = 0.0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epsilon, mode = get_curriculum_epsilon(epoch)\n",
        "\n",
        "        # Train-time PGD step size rule: alpha = epsilon/4 (epsilon=0 ise saldƒ±rƒ± yok)\n",
        "        if epsilon == 0.0:\n",
        "            alpha = 0.0\n",
        "            steps = 0\n",
        "        else:\n",
        "            alpha = epsilon / 4\n",
        "            steps = pgd_steps_train\n",
        "\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if epsilon > 0.0:\n",
        "                # Attack generation with BN stats frozen\n",
        "                model.eval()\n",
        "                x_adv = pgd_attack(\n",
        "                    model, x, y,\n",
        "                    epsilon=epsilon,\n",
        "                    device=device,\n",
        "                    alpha=alpha,\n",
        "                    steps=steps\n",
        "                )\n",
        "                model.train()\n",
        "\n",
        "                # Clear grads from attack gen (safety)\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "\n",
        "                logits = model(x_adv)\n",
        "                loss = F.cross_entropy(logits, y)  # Madry-style\n",
        "            else:\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits, y)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        scheduler.step()\n",
        "\n",
        "        # Clean eval each epoch (fast)\n",
        "        clean_acc = evaluate_clean(model, test_loader)\n",
        "        print(f\"Ep {epoch}/{epochs} | {mode} | Eps: {epsilon:.4f} | LR: {current_lr:.5f} | Loss: {total_loss/len(train_loader):.4f} | Clean: %{clean_acc*100:.2f}\")\n",
        "\n",
        "        # Save best clean\n",
        "        if clean_acc > best_clean_acc:\n",
        "            best_clean_acc = clean_acc\n",
        "            torch.save(model.state_dict(), path_best_clean)\n",
        "\n",
        "        # Robust eval schedule (same as before)\n",
        "        do_eval = (epoch <= 60 and epoch % 10 == 0) or (epoch > 60 and epoch % 5 == 0)\n",
        "        if do_eval:\n",
        "            print(f\"üîé Epoch {epoch} Detaylƒ± Robustness Kontrol√º...\")\n",
        "            _, _, robust_acc = evaluate_detailed(model, test_loader, device)\n",
        "\n",
        "            if robust_acc > best_robust_acc:\n",
        "                best_robust_acc = robust_acc\n",
        "                print(f\"üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%{best_robust_acc*100:.2f}) -> Kaydedildi.\")\n",
        "                torch.save(model.state_dict(), path_best_robust)\n",
        "\n",
        "    # Final save\n",
        "    torch.save(model.state_dict(), path_final)\n",
        "\n",
        "    total_min = (time.time() - start_time) / 60.0\n",
        "    print(f\"\\nüèÅ Eƒüitim Tamamlandƒ±. S√ºre: {total_min:.1f} dk\")\n",
        "    print(f\"En ƒ∞yi Clean Acc: %{best_clean_acc*100:.2f}\")\n",
        "    print(f\"En ƒ∞yi Robust Acc (PGD-20): %{best_robust_acc*100:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"‚úÖ H√úCRE 4 hazƒ±r: WRN-28-10 Curriculum PGD Engine.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ CURRICULUM TRAINING BA≈ûLIYOR: wrn28_10_curriculum_v1\n",
            "   Model: WRN-28-10 | Epochs: 80 | PGD-7 train | PGD-20 eval\n",
            "Ep 1/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 1.4637 | Clean: %54.80\n",
            "Ep 2/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.9902 | Clean: %53.52\n",
            "Ep 3/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.7854 | Clean: %59.55\n",
            "Ep 4/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.6521 | Clean: %71.65\n",
            "Ep 5/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.5736 | Clean: %73.96\n",
            "Ep 6/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.5187 | Clean: %67.70\n",
            "Ep 7/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.4833 | Clean: %82.28\n",
            "Ep 8/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.4491 | Clean: %75.17\n",
            "Ep 9/80 | üü¢ Warm-up (Clean) | Eps: 0.0000 | LR: 0.10000 | Loss: 0.4286 | Clean: %78.84\n",
            "Ep 10/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 1.0027 | Clean: %81.48\n",
            "üîé Epoch 10 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %81.48 | FGSM: %19.87 | PGD-20: %7.45\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%7.45) -> Kaydedildi.\n",
            "Ep 11/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.8706 | Clean: %81.56\n",
            "Ep 12/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.8259 | Clean: %80.42\n",
            "Ep 13/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.7896 | Clean: %80.83\n",
            "Ep 14/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.7594 | Clean: %81.63\n",
            "Ep 15/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.7427 | Clean: %83.12\n",
            "Ep 16/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.7249 | Clean: %81.90\n",
            "Ep 17/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.7100 | Clean: %82.88\n",
            "Ep 18/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.6937 | Clean: %84.18\n",
            "Ep 19/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.6817 | Clean: %83.63\n",
            "Ep 20/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.6662 | Clean: %85.71\n",
            "üîé Epoch 20 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %85.71 | FGSM: %27.12 | PGD-20: %12.75\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%12.75) -> Kaydedildi.\n",
            "Ep 21/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.6627 | Clean: %84.22\n",
            "Ep 22/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.6496 | Clean: %84.57\n",
            "Ep 23/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.6415 | Clean: %85.28\n",
            "Ep 24/80 | üü° Low Noise (2/255) | Eps: 0.0078 | LR: 0.10000 | Loss: 0.6368 | Clean: %86.94\n",
            "Ep 25/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.9603 | Clean: %83.42\n",
            "Ep 26/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.9266 | Clean: %83.03\n",
            "Ep 27/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.9108 | Clean: %83.39\n",
            "Ep 28/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8984 | Clean: %84.24\n",
            "Ep 29/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8881 | Clean: %84.06\n",
            "Ep 30/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8787 | Clean: %84.63\n",
            "üîé Epoch 30 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %84.63 | FGSM: %40.19 | PGD-20: %27.07\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%27.07) -> Kaydedildi.\n",
            "Ep 31/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8777 | Clean: %85.18\n",
            "Ep 32/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8574 | Clean: %85.25\n",
            "Ep 33/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8540 | Clean: %84.04\n",
            "Ep 34/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8556 | Clean: %83.23\n",
            "Ep 35/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8441 | Clean: %85.28\n",
            "Ep 36/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8471 | Clean: %85.51\n",
            "Ep 37/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8376 | Clean: %83.83\n",
            "Ep 38/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8330 | Clean: %85.47\n",
            "Ep 39/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8326 | Clean: %84.20\n",
            "Ep 40/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.10000 | Loss: 0.8234 | Clean: %83.64\n",
            "üîé Epoch 40 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %83.64 | FGSM: %40.83 | PGD-20: %29.44\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%29.44) -> Kaydedildi.\n",
            "Ep 41/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.01000 | Loss: 0.5796 | Clean: %91.13\n",
            "Ep 42/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.01000 | Loss: 0.4860 | Clean: %91.90\n",
            "Ep 43/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.01000 | Loss: 0.4441 | Clean: %91.70\n",
            "Ep 44/80 | üü† Mid Noise (4/255) | Eps: 0.0157 | LR: 0.01000 | Loss: 0.4089 | Clean: %92.02\n",
            "Ep 45/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.6533 | Clean: %91.15\n",
            "Ep 46/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.6137 | Clean: %91.09\n",
            "Ep 47/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.5778 | Clean: %90.75\n",
            "Ep 48/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.5497 | Clean: %91.02\n",
            "Ep 49/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.5209 | Clean: %91.00\n",
            "Ep 50/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.4949 | Clean: %90.78\n",
            "üîé Epoch 50 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %90.78 | FGSM: %58.06 | PGD-20: %43.47\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%43.47) -> Kaydedildi.\n",
            "Ep 51/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.4663 | Clean: %91.06\n",
            "Ep 52/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.4416 | Clean: %90.96\n",
            "Ep 53/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.4195 | Clean: %91.02\n",
            "Ep 54/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.3952 | Clean: %90.89\n",
            "Ep 55/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.3761 | Clean: %90.91\n",
            "Ep 56/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.3540 | Clean: %90.80\n",
            "Ep 57/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.3383 | Clean: %90.98\n",
            "Ep 58/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.3230 | Clean: %90.94\n",
            "Ep 59/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.3058 | Clean: %90.67\n",
            "Ep 60/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.01000 | Loss: 0.2902 | Clean: %90.95\n",
            "üîé Epoch 60 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %90.95 | FGSM: %57.89 | PGD-20: %42.11\n",
            "Ep 61/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.00100 | Loss: 0.2175 | Clean: %91.27\n",
            "Ep 62/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.00100 | Loss: 0.1900 | Clean: %91.30\n",
            "Ep 63/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.00100 | Loss: 0.1794 | Clean: %91.39\n",
            "Ep 64/80 | üî¥ High Noise (6/255) | Eps: 0.0235 | LR: 0.00100 | Loss: 0.1675 | Clean: %91.19\n",
            "Ep 65/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.3644 | Clean: %90.82\n",
            "üîé Epoch 65 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %90.82 | FGSM: %59.97 | PGD-20: %43.90\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%43.90) -> Kaydedildi.\n",
            "Ep 66/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.3512 | Clean: %90.93\n",
            "Ep 67/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.3406 | Clean: %90.77\n",
            "Ep 68/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.3299 | Clean: %90.81\n",
            "Ep 69/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.3217 | Clean: %90.79\n",
            "Ep 70/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.3132 | Clean: %90.76\n",
            "üîé Epoch 70 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %90.76 | FGSM: %60.38 | PGD-20: %43.74\n",
            "Ep 71/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.3058 | Clean: %90.75\n",
            "Ep 72/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2960 | Clean: %90.78\n",
            "Ep 73/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2844 | Clean: %90.55\n",
            "Ep 74/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2816 | Clean: %90.53\n",
            "Ep 75/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2751 | Clean: %90.66\n",
            "üîé Epoch 75 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %90.66 | FGSM: %60.52 | PGD-20: %43.95\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%43.95) -> Kaydedildi.\n",
            "Ep 76/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2706 | Clean: %90.74\n",
            "Ep 77/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2626 | Clean: %90.65\n",
            "Ep 78/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2577 | Clean: %90.57\n",
            "Ep 79/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2515 | Clean: %90.54\n",
            "Ep 80/80 | üî• Target Noise (8/255) | Eps: 0.0314 | LR: 0.00100 | Loss: 0.2446 | Clean: %90.63\n",
            "üîé Epoch 80 Detaylƒ± Robustness Kontrol√º...\n",
            "    üìä [Detaylƒ± Test] Clean: %90.63 | FGSM: %60.42 | PGD-20: %44.12\n",
            "üèÜ YENƒ∞ EN ƒ∞Yƒ∞ ROBUST MODEL! (%44.12) -> Kaydedildi.\n",
            "\n",
            "üèÅ Eƒüitim Tamamlandƒ±. S√ºre: 1101.9 dk\n",
            "En ƒ∞yi Clean Acc: %92.02\n",
            "En ƒ∞yi Robust Acc (PGD-20): %44.12\n"
          ]
        }
      ],
      "source": [
        "# Batch 128 ise:\n",
        "model = train_curriculum_wrn(\n",
        "    model_name=\"wrn28_10_curriculum_v1\",\n",
        "    epochs=80,\n",
        "    lr=0.1,\n",
        "    weight_decay=5e-4,\n",
        "    pgd_steps_train=7\n",
        ")\n",
        "\n",
        "# OOM olursa √∂nce H√úCRE 1‚Äôde get_loaders(batch_size=64) yap,\n",
        "# sonra lr=0.05 ile tekrar dene.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
